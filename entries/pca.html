<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Singular Value Decomposition Pt. II</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css?family=+Sans+Pro:400,300,500,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Frank+Ruhl+Libre:400,300,500,600,200" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,300,500,600,200" rel="stylesheet" type="text/css" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>

    strong {
        font-weight: 500;
    }

    footer a {
      color: #1e90ff;
    }

    footer a:visited {
      color: #1e90ff;
    }

    a {
      text-decoration: none;
    }

    p a {
      color: #1e90ff;
    }

    p a:visited {
      color: #1e90ff;
    }


    a:hover {
      text-decoration: underline;
    }

    nav:before {
        content: "Contents";
        font-weight: bold;
        font-size: 14pt;
        display: text;
        height: 2px;
        width: 100%;
        margin: 10px;
        text-align: center;
        color: #616161;
        opacity: 100%;
        font-family: 'Lato', sans-serif;
        font-weight: normal;
    }

    nav:after {
        content: "";
        display: block;
        height: 2px;
        width: 100%;
        background: black;
        opacity: 50%;
        text-align: center;
    }

    nav *{
      font-family: 'Lato', sans-serif;
      font-weight: normal;
      font-size: 13pt;
    }
    
    nav ul li a, nav ul li span{
        opacity: 75%;
    }

    body {
      text-align: adjust;
      margin: 0 auto;
      max-width: 45em;
      font-family: 'Frank Ruhl Libre', serif;
    }
    html {
      line-height: 1.3;
    }
    h1 {
      font-size: 27pt;
      font-family: 'Lato', sans-serif;
      font-weight: 300;
    }
    h2 {
      font-size: 20pt;
      font-weight: 400;
      font-family: 'Lato', sans-serif;
    }

    h3 {
        font-family: 'Lato', sans-serif;
        font-weight: 500;
    }

    div.csl-entry {
      clear: both;
      margin-bottom: 0.2em;
      font-size: 14pt; 
      opacity: 85%;
    }
    body:after {
      display: block;
      text-align: left;
      font-style: italic;
      opacity: 100;
      font-size: 14pt;
    }

    div.published {
      display: block;
      text-align: right;
      font-style: italic;
      opacity: 60%;
      font-size: 14pt;
    }

    div.home {
      text-align: left;
      font-style: italic;
      font-size: 14pt;
    }

    li[role="doc-endnote"] {
      font-size: 14pt;
    }


    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>The Singular Value Decomposition<br />
Pt. II</p></h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#dimensionality-reduction-via-pca">6. Dimensionality Reduction via PCA</a></li>
<li><a href="#pca-and-variance">7. PCA and Variance</a></li>
<li><a href="#principal-component-regression">8. Principal Component Regression</a></li>
<li><a href="#further-reading">9. Further Reading</a>
<ul>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</nav>
<p>This entry is a follow up to the <a href="svd.html">previous one</a> on the SVD. (Please refer to it for any notation not explicitly defined here.) They were initially going to constitute a single entry, but the resulting length would have been excessive. Here I will assume that, in addition to the requirements of the previous part, the reader is familiar with basic concepts in statistics, such as expected value, variance, correlation, estimation etc.</p>
<h2 id="dimensionality-reduction-via-pca">6. Dimensionality Reduction via PCA</h2>
<p>It is often desirable to project high-dimensional data onto a lower-dimensional space for the purpose of e.g. data visualization (how can we visualize <span class="math inline">\(5\)</span>-dimensional data on a piece of paper?). In mathematical terms, suppose <span class="math inline">\(x_{1}, \ldots, x_{m}\)</span> are vectors in a Euclidean space <span class="math inline">\(\mathbb{R}^{n}\)</span>. The question is: what is the best way to map <span class="math inline">\(x_{1}, \ldots, x_{m}\)</span> into <span class="math inline">\(\mathbb{R}^{k}\)</span> for <span class="math inline">\(k &lt; n\)</span>?</p>
<p>Here “best” needs to be defined. A linear map <span class="math inline">\(\mathbb{R}^{n} \to \mathbb{R}^{k}\)</span> corresponds to a <span class="math inline">\(k \times n\)</span> matrix <span class="math inline">\(U\)</span>, and a linear map <span class="math inline">\(\mathbb{R}^{k} \to \mathbb{R}^{n}\)</span> to an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(V\)</span>. The matrix <span class="math inline">\(VU\)</span> thus corresponds to a linear map that first sends <span class="math inline">\(\mathbb{R}^{n}\)</span> to <span class="math inline">\(\mathbb{R}^{k}\)</span> and then <span class="math inline">\(\mathbb{R}^{k}\)</span> back to <span class="math inline">\(\mathbb{R}^n\)</span>. “Best” will mean a choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> that minimizes the objective function <span class="math display">\[\mathcal{F}_{k}(U, V) = \sum_{i=1}^{m} \|x_i - VUx_i\|^{2}.\]</span> If <span class="math inline">\(x_1, \ldots, x_m\)</span> actually represent the rows of the matrix <span class="math inline">\(X\)</span>, the above can be rewritten as <span class="math display">\[\mathcal{F}_k(U, V) = \|X - X U^{\intercal} V^{\intercal}\|^2\]</span> in terms of the matrix norm (why?). It is clear that if <span class="math inline">\(r &lt; k\)</span>, then <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> can be chosen so that <span class="math inline">\(X U^{\intercal} V^{\intercal} = X\)</span>, making the question trivial. We therefore assume <span class="math inline">\(k \leq r\)</span>.</p>
<p>Now, the matrix <span class="math inline">\(X U^{\intercal} V^{\intercal}\)</span> has rank at most <span class="math inline">\(k\)</span> and according to the EYM theorem, <span class="math display">\[\begin{align*}
\|X - X_k\|^2 &amp;\leq\mathcal{F}(U,V) \\
                 &amp;= \|X - X U^{\intercal} V^{\intercal}\|^2.
\end{align*}\]</span> But actually, we can choose <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> so that <span class="math inline">\(X_k = X U^{\intercal} V^{\intercal}\)</span>! Simply set <span class="math display">\[U^{\intercal} = [Q_{ij}]_{i \leq k} [\Sigma_{ij}]_{i,j\leq k}^{-1} \ \ \text{and}\]</span> <span class="math display">\[V^{\intercal} = [\Sigma_{ij}]_{i,j\leq k} [Q_{ij}]_{i \leq k}^{\intercal}.\]</span> To see this, observe that following the characterization of <span class="math inline">\(P\)</span> derived in §1, <span class="math display">\[X [Q_{ij}]_{j \leq k} [\Sigma_{ij}]_{i,j\leq k}^{-1} = [P_{ij}]_{j\leq k},\]</span> and so with this choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> we have <span class="math display">\[XU^{\intercal} V^{\intercal} = [P_{ij}]_{j\leq k} [\Sigma_{ij}]_{i,j\leq k} [Q_{ij}]_{j \leq k}^{\intercal},\]</span> which is precisely the matrix <span class="math inline">\(X_k\)</span>. The matrix <span class="math inline">\([\Sigma_{ij}]_{i,j\leq k}\)</span> actually cancels out with its inverse in the product <span class="math inline">\(U^{\intercal} V^{\intercal}\)</span>, so we can simplify our choice to <span class="math inline">\(U^{\intercal} = [Q_{ij}]_{j\leq k}\)</span> and <span class="math inline">\(V^{\intercal} = [Q_{ij}]_{j\leq k}^{\intercal}\)</span>. We therefore established</p>
<div class="prop*">
<p><strong>Proposition</strong>. <em>The choice <span class="math inline">\(U = [Q_{ij}]_{j\leq k}^{\intercal}\)</span> and <span class="math inline">\(V = [Q_{ij}]_{j\leq k}\)</span> minimizes <span class="math inline">\(\mathcal{F}_k(U,V)\)</span>.</em></p>
</div>
<p>Conceptually, the best way to map the vectors <span class="math inline">\(x_1, \ldots, x_m\)</span> into <span class="math inline">\(\mathbb{R}^{k}\)</span> turns out to first apply a certain orthogonal transformation to the vectors and then drop the last <span class="math inline">\(n - k\)</span> dimensions. The vector <span class="math inline">\(q_i\)</span> (the <span class="math inline">\(i^{\text{th}}\)</span> row of <span class="math inline">\(Q^{\intercal}\)</span>) is called the <em><span class="math inline">\(i^{\text{th}}\)</span> principal component</em> of the data <span class="math inline">\(x_1, \ldots, x_m\)</span>, hence <em>principal component analysis</em> (PCA).</p>
<p>Note that when <span class="math inline">\(k = r\)</span>, the obvious optimal choice for <span class="math inline">\(U\)</span> is simply some truncated permutation of the identity matrix, yet the one given above is <span class="math inline">\(Q^{\intercal}\)</span>, which might seem unnecessary. However, from a statistical perspective, this has the benefit of decorrelating the data. More on this in the next section.</p>
<h2 id="pca-and-variance">7. PCA and Variance</h2>
<p>Suppose now the vectors <span class="math inline">\(x_1, \ldots, x_{m}\)</span> introduced above correspond to <span class="math inline">\(n\)</span> observations across <span class="math inline">\(m\)</span> separate random variables. Assume these variables have been centered so that they each have zero in-sample mean. (From here on, I will omit the prefix <em>in-sample</em> but it is implied for all the statistical concepts appearing in this section.)</p>
<p>Consider finding a direction of greatest variance within our multi-dimensional data, that is to say a ray through the origin for which the random variable given by the scalar projections of the vectors <span class="math inline">\(x_1, \ldots, x_m\)</span> onto that ray has the greatest possible variance. More concretely, we are looking for a unit vector <span class="math inline">\(u \in \mathbb{R}^{m}\)</span> that maximizes the variance of <span class="math inline">\(Xu\)</span>. Since <span class="math inline">\(Xu\)</span> is also centered (why?), this variance is equal to <span class="math display">\[\tfrac{1}{n} \|Xu\|^{2} = \tfrac{1}{n} u^{\intercal} X^{\intercal} X u.\]</span> We know that <span class="math inline">\(X^{\intercal} X\)</span> is orthogonally diagonalized by the matrix <span class="math inline">\(Q\)</span>. Expanding <span class="math inline">\(X^{\intercal} X\)</span> in terms of this diagonalization, we readily see that <span class="math inline">\(u\)</span> maximizes the above quantity iff it is an eigenvector of the largest eigenvalue of <span class="math inline">\(X^{\intercal}X\)</span>, which is <span class="math inline">\(\sigma_1^{2}\)</span>. Thus the principal component <span class="math inline">\(q_1\)</span> works as a choice for <span class="math inline">\(u\)</span>.</p>
<p>Having chosen <span class="math inline">\(u\)</span>, we can then ask for a direction of greatest variance <em>that produces a random variable uncorrelated with <span class="math inline">\(Xu\)</span></em>. So we wish to find another unit vector <span class="math inline">\(\tilde{u}\)</span> that maximizes the variance of <span class="math inline">\(X\tilde{u}\)</span> subject to that random variable having zero covariance with <span class="math inline">\(Xu\)</span>. Actually, this covariance is given by <span class="math display">\[\begin{align*}
\tfrac{1}{n} \langle Xu, X\tilde{u} \rangle &amp;= \tfrac{1}{n} \tilde{u}^{\intercal} X^{\intercal} X u \\
                                  &amp;= \tfrac{1}{n} \sigma_1^{2}\langle u, \tilde{u} \rangle,
\end{align*}\]</span> so this simply means that <span class="math inline">\(u\)</span> and <span class="math inline">\(\tilde{u}\)</span> must be orthogonal. Following this observation, we similarly see that the second principal component <span class="math inline">\(q_2\)</span> works as choice for <span class="math inline">\(\tilde{u}\)</span>. And so on, so that the principal components optimize the iterative process of finding directions of greatest variance one at a time subject to the resulting random variables being mutually uncorrelated.</p>
<p>The sum of the variances of the columns of <span class="math inline">\(X\)</span>, that is the trace of the covariance matrix <span class="math inline">\(\tfrac{1}{n} X^{\intercal} X\)</span>, is known as the <em>total variance</em> of the data. This quantity is left unchanged when multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(Q\)</span>, and in fact is equal to <span class="math inline">\(\tfrac{1}{n}\sum_i \sigma_i^{2}\)</span> (why?). As <span class="math inline">\(Xq_j\)</span> has variance <span class="math inline">\(\tfrac{1}{n} \sigma_j^{2}\)</span>, it is said to “explain” <span class="math inline">\(\sigma^{2}_j (\sum_i \sigma_i^{2})^{-1}\)</span> of the variance present in the data.</p>
<h2 id="principal-component-regression">8. Principal Component Regression</h2>
<p>Consider again the problem of linear regression by ordinary least squares (OLS): find a <span class="math inline">\(w\)</span> that minimizes <span class="math inline">\(\|Xw - y\|\)</span> for some vector <span class="math inline">\(y\)</span>. As we saw, a solution is <span class="math inline">\(w = X^{+} y\)</span>. When <span class="math inline">\(X^{\intercal} X\)</span> is invertible, the pseudoinverse <span class="math inline">\(X^{+}\)</span> is easily computed. When it isn’t, we may discard columns of <span class="math inline">\(X\)</span> until it is, never mind if this poses a problem for explainability. Whatever the case, there will remain a fundamental difficulty: if <span class="math inline">\(X^{\intercal} X\)</span> has eigenvalues close (but not necessarily equal) to zero, there will be great danger of overfitting.</p>
<p>To understand this last statement, consider the following setting. Suppose the process underlying the response <span class="math inline">\(y\)</span> truly is linear in the columns of <span class="math inline">\(X\)</span> but that, due to imprecise measuring instruments perhaps, some degree of randomness exists in the actual observed values of <span class="math inline">\(y\)</span>. We can then write <span class="math display">\[y = X \widetilde{w} + \epsilon,\]</span> where <span class="math inline">\(\widetilde{w}\)</span> is a fixed vector and <span class="math inline">\(\epsilon\)</span> is a random vector representing the error between the true and observed values of <span class="math inline">\(y\)</span>. Assume that (a) the expected value of each component of <span class="math inline">\(\epsilon\)</span> is zero (so no systematic errors) and (b) the variance of each component of <span class="math inline">\(\epsilon\)</span> is the same; this is known as <em>homoscedasticity</em> and can be interpreted as stating that the source of error for each observed component of <span class="math inline">\(y\)</span> is the same. More succinctly, the covariance matrix of <span class="math inline">\(\epsilon\)</span> is <span class="math inline">\(\mu^{2} I\)</span>, where <span class="math inline">\(\mu^2\)</span> is the common variance of the components of <span class="math inline">\(\epsilon\)</span>.</p>
<p>Given the above, we would estimate <span class="math inline">\(\widetilde{w}\)</span> as <span class="math display">\[\begin{align*}
w &amp;= X^{+}(X\widetilde{w} + \epsilon) \\
  &amp;= X^{+} X \widetilde{w} + X^{+} \epsilon.
\end{align*}\]</span> The expected value of <span class="math inline">\(w\)</span> is thus <span class="math inline">\(X^{+} X w\)</span>, making it an unbiased estimator of <span class="math inline">\(\widetilde{w}\)</span> when <span class="math inline">\(X^{\intercal} X\)</span> is invertible. The expected fitted value on the other hand is <span class="math display">\[\underbrace{XX^{+}X}_{X} \widetilde{w} +\! \cancel{E[XX^{+} \epsilon]} = y.\]</span> Finally, the covariance matrix of <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{align*}
  E[X^{+} \epsilon \, (X^{+} \epsilon)^{\intercal}] &amp;=  E[X^{+} \epsilon \, \epsilon^{\intercal} (X^{+})^{\intercal}] \\
&amp;= \mu^{2} X^{+} (X^{+})^{\intercal} \\
&amp;= \mu^{2} Q \Sigma^{+} (\Sigma^{+})^{\intercal} Q^{\intercal}, 
\end{align*}\]</span> where homoscedasticity was used in the second equality. Thus the total variance of <span class="math inline">\(w\)</span> is linear in the reciprocals of the squares of the non-zero singular values of <span class="math inline">\(X\)</span>. If the last few of these are close to zero, then this total variance will tend to be very large. This is problematic because it means the estimator <span class="math inline">\(w\)</span> will be very sensitive to the error <span class="math inline">\(\epsilon\)</span>, easily leading to overfitting. As vanishing singular values indicate the presence of collinearities among the columns of <span class="math inline">\(X\)</span>, small non-zero singular values can be seen as indicating the presence of “near-collinearities”.</p>
<p>This can be remedied by appealing to the principal components of <span class="math inline">\(X\)</span>. In short, the predictors (the columns of <span class="math inline">\(X\)</span>) are replaced by the decorrelated variables <span class="math inline">\(Xq_1, \ldots, Xq_{k}\)</span> for <span class="math inline">\(k \leq r\)</span>, OLS linear regression is performed with respect to these and finally, if need be, the initial predictors are reverted to. This is called <em>principal component regression</em> (PCR).</p>
<p>Concretely, we solve the problem of finding a <span class="math inline">\(w&#39;_k\)</span> that minimizes <span class="math inline">\(\|X [Q_{ij}]_{j\leq k}\, w&#39;_k - y\|\)</span>. A solution is <span class="math display">\[\begin{align*}
w&#39;_k &amp;= (X [Q_{ij}]_{j\leq k})^{+} y \\
&amp;= [\Sigma^{+}_{ij}]_{i\leq k} \, P^{\intercal} \, y.
\end{align*}\]</span></p>
<p>The expected value of <span class="math inline">\(w&#39;_k\)</span> is <span class="math inline">\([Q_{ij}]_{j\leq k}^{\intercal} \widetilde{w}\)</span> and its associated expected fitted value is <span class="math inline">\(X X_{k}^{+} X y\)</span>. The covariance matrix of <span class="math inline">\(w&#39;_{r}\)</span> is</p>
<p><span class="math display">\[\mu^{2} \Sigma^{+} (\Sigma^{+})^{\intercal}.\]</span> Furthermore, the effect of reducing <span class="math inline">\(k\)</span> below <span class="math inline">\(r\)</span> is simply to trim away the last <span class="math inline">\(r - k\)</span> entries of <span class="math inline">\(w&#39;_{r}\)</span>, so the <span class="math inline">\(i^{\text{th}}\)</span> component of <span class="math inline">\(w&#39;_k\)</span> (for <span class="math inline">\(i \leq k\)</span>) is independent of <span class="math inline">\(k\)</span> and its variance is <span class="math inline">\(\mu^2 \sigma_i^{-2}\)</span>. This last observation is important because it tells us that (a) we need not carry out <span class="math inline">\(r\)</span> different calculations to compute <span class="math inline">\(w&#39;_1,\)</span> <span class="math inline">\(\ldots,\)</span> <span class="math inline">\(w&#39;_r\)</span>, and (b) total variance can be monotonically reduced by keeping only the first few principal components of <span class="math inline">\(X\)</span>.</p>
<p>In terms of the initial predictors, retaining the first <span class="math inline">\(k\)</span> principal components is akin to using the estimator <span class="math inline">\([Q_{ij}]_{j\leq k} \, w&#39;_k\)</span> for <span class="math inline">\(\widetilde{w}\)</span>. However, unless <span class="math inline">\(X^{\intercal} X\)</span> is invertible and <span class="math inline">\(k = r\)</span>, it will generally not be unbiased, with bias equal to <span class="math display">\[[Q_{ij}]_{j\leq k} [Q_{ij}]_{j\leq k}^{\intercal}\widetilde{w} - \widetilde{w}.\]</span> Conceptually, this is the error incurred on <span class="math inline">\(\widetilde{w}\)</span> by the dimensionality reduction procedure seen in §6. The total variance of this estimator on the other hand is <span class="math display">\[\mu^2 \sum_{i=1}^{k} \sigma_i^{-2}.\]</span> As <span class="math inline">\(k\)</span> decreases so will total variance but, as one might imagine, bias will generally increase. This is an instance of the perennial bias-variance tradeoff. That said, in practice some level of bias is preferable to a large total variance.</p>
<p>Another boon of using PCR is that the components of <span class="math inline">\([Q_{ij}]_{j\leq k} \, w&#39;_k\)</span> will in general not vanish, meaning all the initial predictors are involved even when <span class="math inline">\(X^{\intercal} X\)</span> is not invertible. This is often better for explainability.</p>
<h2 id="further-reading">9. Further Reading</h2>
<p>The book <span class="citation" data-cites="pca">[4]</span> offers a comprehensive coverage of PCA, addressing a mix of theoretical and practical questions.</p>
<p>For more theoretical results on the SVD, <span class="citation" data-cites="matrix">[3]</span> is good reference.</p>
<p>The book <span class="citation" data-cites="manning">[5]</span> and the primary sources <span class="citation" data-cites="berry index">[1, 2]</span> discuss the SVD in the context of latent semantic analysis/indexing. The reference <span class="citation" data-cites="berry">[1]</span> furthermore contains expressions for the time complexities of computing the SVD, updating the SVD, and folding-in.</p>
<p>See <span class="citation" data-cites="rec rec2">[6, 7]</span> for the use of the SVD in collaborative filtering in the context of recommender systems.</p>
<p>The fundamental result explaining why estimators with lower total variance than the OLS estimator must necessarily be biased is the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a>. Ridge regression provides an alternative to PCR in producing such estimators. Some details from that perspective are given at the link <span class="citation" data-cites="taboga">[8]</span>.</p>
<h3 class="unnumbered" id="references">References</h3>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-berry" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">Berry, M.W. et al. 1995. Using linear algebra for intelligent information retrieval. <em>SIAM review</em>. 37, 4 (1995), 573–595.</div>
</div>
<div id="ref-index" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Deerwester, S. et al. 1990. Indexing by latent semantic analysis. <em>Journal of the American society for information science</em>. 41, 6 (1990), 391–407.</div>
</div>
<div id="ref-matrix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Horn, R.A. and Johnson, C.R. 2012. <em>Matrix analysis</em>. Cambridge University Press.</div>
</div>
<div id="ref-pca" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Jolliffe, I.T. 2002. <em>Principal component analysis</em>. Springer.</div>
</div>
<div id="ref-manning" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Manning, C. 2008. <em>Introduction to information retrieval</em>. Cambridge University Press.</div>
</div>
<div id="ref-rec" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Sarwar, B. et al. 2000. <em>Application of dimensionality reduction in recommender system-a case study</em>. Minnesota Univ Minneapolis Dept of Computer Science.</div>
</div>
<div id="ref-rec2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Sarwar, B. et al. 2002. Incremental singular value decomposition algorithms for highly scalable recommender systems. <em>Fifth international conference on computer and information science</em> (2002), 27–8.</div>
</div>
<div id="ref-taboga" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Taboga, M. 2021. <a href="https://www.statlect.com/fundamentals-of-statistics/ridge-regression">Ridge regression</a>. <em>Lectures on probability theory and mathematical statistics</em>. Kindle Direct Publishing. Online appendix.</div>
</div>
</div>
<footer>
  <div class="container" style="display: flex; margin-top: 50px">
      <div class="home" width=50%>
        <a href="../index.html">Home</a>
      </div>
      <div class="published" style="flex-grow: 1; text-align: right">
      Published on Mon March 21, 2022
      </div>
  </div>
</footer>
</body>
</html>
