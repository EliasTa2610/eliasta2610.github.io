<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Singular Value Decomposition Pt. II</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css?family=+Sans+Pro:400,300,500,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Frank+Ruhl+Libre:400,300,500,600,200" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,300,500,600,200" rel="stylesheet" type="text/css" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>

    strong {
        font-weight: 500;
    }

    footer a {
      color: #1e90ff;
    }

    footer a:visited {
      color: #1e90ff;
    }

    a {
      text-decoration: none;
    }

    p a {
      color: #1e90ff;
    }

    p a:visited {
      color: #1e90ff;
    }


    a:hover {
      text-decoration: underline;
    }

    nav:before {
        content: "Contents";
        font-weight: bold;
        font-size: 14pt;
        display: text;
        height: 2px;
        width: 100%;
        margin: 10px;
        text-align: center;
        color: #616161;
        opacity: 100%;
        font-family: 'Lato', sans-serif;
        font-weight: normal;
    }

    nav:after {
        content: "";
        display: block;
        height: 2px;
        width: 100%;
        background: black;
        opacity: 50%;
        text-align: center;
    }

    nav *{
      font-family: 'Lato', sans-serif;
      font-weight: normal;
      font-size: 13pt;
    }
    
    nav ul li a, nav ul li span{
        opacity: 75%;
    }

    body {
      text-align: adjust;
      margin: 0 auto;
      max-width: 45em;
      font-family: 'Frank Ruhl Libre', serif;
    }
    html {
      line-height: 1.3;
    }
    h1 {
      font-size: 27pt;
      font-family: 'Lato', sans-serif;
      font-weight: 300;
    }
    h2 {
      font-size: 20pt;
      font-weight: 400;
      font-family: 'Lato', sans-serif;
    }

    h3 {
        font-family: 'Lato', sans-serif;
        font-weight: 500;
    }

    div.csl-entry {
      clear: both;
      margin-bottom: 0.2em;
      font-size: 14pt; 
      opacity: 85%;
    }
    body:after {
      display: block;
      text-align: left;
      font-style: italic;
      opacity: 100;
      font-size: 14pt;
    }

    div.published {
      display: block;
      text-align: right;
      font-style: italic;
      opacity: 60%;
      font-size: 14pt;
    }

    div.home {
      text-align: left;
      font-style: italic;
      font-size: 14pt;
    }

    li[role="doc-endnote"] {
      font-size: 14pt;
    }


    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>The Singular Value Decomposition<br />
Pt. II</p></h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#dimensionality-reduction-via-pca">6. Dimensionality Reduction via PCA</a></li>
<li><a href="#pca-and-variance">7. PCA and Variance</a></li>
<li><a href="#near-collinearities">8. Near-Collinearities</a></li>
<li><a href="#principal-component-regression">9. Principal Component Regression</a></li>
<li><a href="#ridge-regression">10. Ridge Regression</a></li>
<li><a href="#further-reading">11. Further Reading</a>
<ul>
<li><a href="#references">References</a></li>
</ul></li>
</ul>
</nav>
<p>This entry is a follow up to the [previous one](svd.html) on the SVD. (Please refer to it for any notation not explicitly defined here.) They were initially going to constitute a single entry, but the resulting length would have been excessive. Here I will assume that, in addition to the requirements of the previous part, the reader is familiar with basic concepts in statistics, such as expected value, variance, correlation, estimation etc.</p>
<h2 id="dimensionality-reduction-via-pca">6. Dimensionality Reduction via PCA</h2>
<p>It is often desirable to project high-dimensional data onto a lower-dimensional space for the purpose of e.g. data visualization (how can we visualize <span class="math inline">\(5\)</span>-dimensional data on a piece of paper?). In mathematical terms, suppose <span class="math inline">\(x_{1}, \ldots, x_{m}\)</span> are vectors in a Euclidean space <span class="math inline">\(\mathbb{R}^{n}\)</span>. The question is: what is the best way to map <span class="math inline">\(x_{1}, \ldots, x_{m}\)</span> into <span class="math inline">\(\mathbb{R}^{k}\)</span> for <span class="math inline">\(k &lt; n\)</span>?</p>
<p>Here “best” needs to be defined. A linear map <span class="math inline">\(\mathbb{R}^{n} \to \mathbb{R}^{k}\)</span> corresponds to a <span class="math inline">\(k \times n\)</span> matrix <span class="math inline">\(U\)</span>, and a linear map <span class="math inline">\(\mathbb{R}^{k} \to \mathbb{R}^{n}\)</span> to an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(V\)</span>. The matrix <span class="math inline">\(VU\)</span> thus corresponds to a linear map that first sends <span class="math inline">\(\mathbb{R}^{n}\)</span> to <span class="math inline">\(\mathbb{R}^{k}\)</span> and then <span class="math inline">\(\mathbb{R}^{k}\)</span> back to <span class="math inline">\(\mathbb{R}^n\)</span>. “Best” will mean a choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> that minimizes the objective function <span class="math display">\[\mathcal{F}_{k}(U, V) = \sum_{i=1}^{m} \|x_i - VUx_i\|^{2}.\]</span> If <span class="math inline">\(x_1, \ldots, x_m\)</span> actually represent the rows of the matrix <span class="math inline">\(X\)</span>, the above can be rewritten as <span class="math display">\[\mathcal{F}_k(U, V) = \|X - X U^{\intercal} V^{\intercal}\|^2\]</span> in terms of the matrix norm (why?). It is clear that if <span class="math inline">\(r &lt; k\)</span>, then <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> can be chosen so that <span class="math inline">\(X U^{\intercal} V^{\intercal} = X\)</span>, making the question trivial. We therefore assume <span class="math inline">\(k \leq r\)</span>.</p>
<p>Now, the matrix <span class="math inline">\(X U^{\intercal} V^{\intercal}\)</span> has rank at most <span class="math inline">\(k\)</span> and according to the EYM theorem, <span class="math display">\[\begin{align*}
\|X - X_k\|^2 &amp;\leq\mathcal{F}(U,V) \\
                 &amp;= \|X - X U^{\intercal} V^{\intercal}\|^2.
\end{align*}\]</span> But actually, we can choose <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> so that <span class="math inline">\(X_k = X U^{\intercal} V^{\intercal}\)</span>! Simply set <span class="math display">\[U^{\intercal} = [Q_{ij}]_{j \leq k} [\Sigma_{ij}]_{i,j\leq k}^{-1} \ \ \text{and}\]</span> <span class="math display">\[V^{\intercal} = [\Sigma_{ij}]_{i,j\leq k} [Q_{ij}]_{i \leq k}^{\intercal}.\]</span> To see this, observe that following the characterization of <span class="math inline">\(P\)</span> derived in §1, <span class="math display">\[X [Q_{ij}]_{j \leq k} [\Sigma_{ij}]_{i,j\leq k}^{-1} = [P_{ij}]_{j\leq k},\]</span> and so with this choice of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> we have <span class="math display">\[XU^{\intercal} V^{\intercal} = [P_{ij}]_{j\leq k} [\Sigma_{ij}]_{i,j\leq k} [Q_{ij}]_{j \leq k}^{\intercal},\]</span> which is precisely the matrix <span class="math inline">\(X_k\)</span>. The matrix <span class="math inline">\([\Sigma_{ij}]_{i,j\leq k}\)</span> actually cancels out with its inverse in the product <span class="math inline">\(U^{\intercal} V^{\intercal}\)</span>, so we can simplify our choice to <span class="math inline">\(U^{\intercal} = [Q_{ij}]_{j\leq k}\)</span> and <span class="math inline">\(V^{\intercal} = [Q_{ij}]_{j\leq k}^{\intercal}\)</span>. We therefore established</p>
<div class="prop*">
<p><strong>Proposition* 1</strong>. <em>The choice <span class="math inline">\(U = [Q_{ij}]_{j\leq k}^{\intercal}\)</span> and <span class="math inline">\(V = [Q_{ij}]_{j\leq k}\)</span> minimizes <span class="math inline">\(\mathcal{F}_k(U,V)\)</span>.</em></p>
</div>
<p>Conceptually, the best way to map the vectors <span class="math inline">\(x_1, \ldots, x_m\)</span> into <span class="math inline">\(\mathbb{R}^{k}\)</span> turns out to first apply a certain orthogonal transformation to the vectors and then drop the last <span class="math inline">\(n - k\)</span> dimensions. The vector <span class="math inline">\(q_i\)</span> (the <span class="math inline">\(i^{\text{th}}\)</span> row of <span class="math inline">\(Q^{\intercal}\)</span>) is called the <em><span class="math inline">\(i^{\text{th}}\)</span> principal component</em> of the data <span class="math inline">\(x_1, \ldots, x_m\)</span>, hence <em>principal component analysis</em> (PCA).</p>
<p>Note that when <span class="math inline">\(k = r\)</span>, the obvious optimal choice for <span class="math inline">\(U\)</span> is simply some truncated permutation of the identity matrix, yet the one given above is <span class="math inline">\(Q^{\intercal}\)</span>, which might seem unnecessary. However, from a statistical perspective, this has the benefit of decorrelating the data. More on this in the next section.</p>
<h2 id="pca-and-variance">7. PCA and Variance</h2>
<p>Suppose now the vectors <span class="math inline">\(x_1, \ldots, x_{m}\)</span> introduced above correspond to <span class="math inline">\(m\)</span> observations across <span class="math inline">\(n\)</span> separate random variables. Assume these variables have been centered so that they each have zero in-sample mean. (From here on, I will omit the prefix <em>in-sample</em> but it is implied for all the statistical concepts appearing in this section.)</p>
<p>Consider finding a direction of greatest variance within our multi-dimensional data, that is to say a ray through the origin for which the random variable given by the scalar projections of the vectors <span class="math inline">\(x_1, \ldots, x_m\)</span> onto that ray has the greatest possible variance. More concretely, we are looking for a unit vector <span class="math inline">\(u \in \mathbb{R}^{m}\)</span> that maximizes the variance of <span class="math inline">\(Xu\)</span>. Since <span class="math inline">\(Xu\)</span> is also centered (why?), this variance is equal to <span class="math display">\[\tfrac{1}{n} \|Xu\|^{2} = \tfrac{1}{n} u^{\intercal} X^{\intercal} X u.\]</span> We know that <span class="math inline">\(X^{\intercal} X\)</span> is orthogonally diagonalized by the matrix <span class="math inline">\(Q\)</span>. Expanding <span class="math inline">\(X^{\intercal} X\)</span> in terms of this diagonalization, we readily see that <span class="math inline">\(u\)</span> maximizes the above quantity iff it is an eigenvector of the largest eigenvalue of <span class="math inline">\(X^{\intercal}X\)</span>, which is <span class="math inline">\(\sigma_1^{2}\)</span>. Thus the principal component <span class="math inline">\(q_1\)</span> works as a choice for <span class="math inline">\(u\)</span>.</p>
<p>Having chosen <span class="math inline">\(u\)</span>, we can then ask for a direction of greatest variance <em>that produces a random variable uncorrelated with <span class="math inline">\(Xu\)</span></em>. So we wish to find another unit vector <span class="math inline">\(\tilde{u}\)</span> that maximizes the variance of <span class="math inline">\(X\tilde{u}\)</span> subject to that random variable having zero covariance with <span class="math inline">\(Xu\)</span>. Actually, this covariance is given by <span class="math display">\[\begin{align*}
\tfrac{1}{n} \langle Xu, X\tilde{u} \rangle &amp;= \tfrac{1}{n} \tilde{u}^{\intercal} X^{\intercal} X u \\
                                    &amp;= \tfrac{1}{n} \sigma_1^{2}\langle u, \tilde{u} \rangle,
\end{align*}\]</span> so this simply means that <span class="math inline">\(u\)</span> and <span class="math inline">\(\tilde{u}\)</span> must be orthogonal. Following this observation, we similarly see that the second principal component <span class="math inline">\(q_2\)</span> works as choice for <span class="math inline">\(\tilde{u}\)</span>. And so on, so that the principal components optimize the iterative process of finding directions of greatest variance one at a time subject to the resulting random variables being mutually uncorrelated.</p>
<p>Recall that the total variance of a random vector is the trace of its covariance matrix. So the total variance in the present context is <span class="math inline">\(\tfrac{1}{n} \mathop{\mathrm{Tr}}( X^{\intercal} X)\)</span>. Basic properties of the trace imply that this quantity is left unchanged when replacing <span class="math inline">\(X\)</span> by <span class="math inline">\(X Q\)</span>, and in fact is equal to <span class="math inline">\(\tfrac{1}{n}\sum_i \sigma_i^{2}\)</span>. As <span class="math inline">\(Xq_j\)</span> has variance <span class="math inline">\(\tfrac{1}{n} \sigma_j^{2}\)</span>, it is said to “explain” <span class="math inline">\(\sigma^{2}_j (\sum_i \sigma_i^{2})^{-1}\)</span> of the variance present in the data.</p>
<h2 id="near-collinearities">8. Near-Collinearities</h2>
<p>Consider again the problem of linear regression by ordinary least squares (OLS): find a <span class="math inline">\(w\)</span> that minimizes <span class="math inline">\(\|Xw - y\|\)</span> for some vector <span class="math inline">\(y\)</span>. As we saw, a solution is <span class="math inline">\(w = X^{+} y\)</span>. When <span class="math inline">\(X^{\intercal} X\)</span> is invertible, the pseudoinverse <span class="math inline">\(X^{+}\)</span> is easily computed. When it is not, we may discard columns of <span class="math inline">\(X\)</span> until it is, never mind the issue of explainability. Whatever the case, there will remain a fundamental difficulty: if <span class="math inline">\(X^{\intercal} X\)</span> has eigenvalues close (but not necessarily equal) to zero, there will be great danger of overfitting.</p>
<p>To understand this last statement, consider the following setting. Suppose the process underlying the response <span class="math inline">\(y\)</span> truly is linear in the columns of <span class="math inline">\(X\)</span> but that, due to imprecise measuring instruments perhaps, some degree of randomness exists in the actual observed values of <span class="math inline">\(y\)</span>. We can then write <span class="math display">\[y = X w + \epsilon,\]</span> where <span class="math inline">\(w\)</span> is a fixed vector and <span class="math inline">\(\epsilon\)</span> is a random vector representing the error between the true and observed values of <span class="math inline">\(y\)</span>. Assume that (a) the expected value of each component of <span class="math inline">\(\epsilon\)</span> is zero (so no systematic errors) and (b) the variance of each component of <span class="math inline">\(\epsilon\)</span> is the same; this is known as <em>homoscedasticity</em> and can be interpreted as stating that the source of error for each observed component of <span class="math inline">\(y\)</span> is the same. More succinctly, the covariance matrix of <span class="math inline">\(\epsilon\)</span> is <span class="math inline">\(\mu^{2} I\)</span>, where <span class="math inline">\(\mu^2\)</span> is the common variance of the components of <span class="math inline">\(\epsilon\)</span>.</p>
<p>Given the above statistical model, the for <span class="math inline">\(w\)</span> is <span class="math display">\[\begin{align*}
  \widehat{w} &amp;= X^{+}(Xw + \epsilon) \\
  &amp;= X^{+} X w + X^{+} \epsilon.
\end{align*}\]</span> The expected value of the OLS estimator is <span class="math inline">\(X^{+} X w\)</span>, making it an unbiased estimator of <span class="math inline">\(w\)</span> when <span class="math inline">\(X^{\intercal} X\)</span> is invertible. The expected fitted value on the other hand is <span class="math display">\[\underbrace{XX^{+}X}_{X} w +\! \cancel{E[XX^{+} \epsilon]} = Xw.\]</span> Finally, the covariance matrix of <span class="math inline">\(\widehat{w}\)</span> is <span class="math display">\[\begin{align*}
  E[X^{+} \epsilon \, (X^{+} \epsilon)^{\intercal}] &amp;=  E[X^{+} \epsilon \, \epsilon^{\intercal} (X^{+})^{\intercal}] \\
&amp;= \mu^{2} X^{+} (X^{+})^{\intercal} \\
&amp;= \mu^{2} Q \Sigma^{+} (\Sigma^{+})^{\intercal} Q^{\intercal}, 
\end{align*}\]</span> where homoscedasticity was used in the second equality. Thus the total variance of <span class="math inline">\(\widehat{w}\)</span> is <span class="math display">\[\mu^{2} \sum_{i \leq r} \sigma_i^{-2}.\]</span> If the last few non-zero singular values are close to zero, the total variance will tend to be very large. This is problematic because it means the estimator <span class="math inline">\(\widehat{w}\)</span> will be very sensitive to the error <span class="math inline">\(\epsilon\)</span>, easily leading to overfitting. As vanishing singular values indicate the presence of collinearities among the columns of <span class="math inline">\(X\)</span>, small non-zero singular values can be seen as indicating the presence of “near-collinearities”.</p>
<p>This last point can be better appreciated by considering the error that an estimator incurs on balance across all values of <span class="math inline">\(y\)</span>. The <em>mean squared error</em> (MSE) of an estimator <span class="math inline">\(e(\cdot \, ; X, w): \mathbb{R}^{n} \to \mathbb{R}^{m}\)</span> is the expected value of <span class="math inline">\(\|e(y \, ; X, w) - w\|^{2}\)</span> as <span class="math inline">\(y\)</span> is allowed to vary. It decomposes as the sum of the total variance of the estimator and the square of its normed bias: <span class="math display">\[\begin{gathered}
E[\|e(y \, ; X, w) - w\|^{2}] = \\\
\|E[e(y \,; X, w)] - w\|^2 + \mathop{\mathrm{Tr}}(\mathop{\mathrm{Cov}}(e(y \, ; X, w)).\end{gathered}\]</span> Though the first summand on the RHS may be small (in fact <span class="math inline">\(0\)</span> for the OLS estimator when <span class="math inline">\(X^{\intercal} X\)</span> is invertible), the second may be unacceptably large when near-collinearities are present.</p>
<p>The [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) states that for estimators linear in <span class="math inline">\(y\)</span> (the only kind considered here), there is a fundamental tension between the two summands. That is, assuming <span class="math inline">\(X^{\intercal} X\)</span> is invertible, the OLS estimator has the lowest total variance among all unbiased linear estimators. Thus any estimator with lower MSE will necessarily be biased. In the sequel we consider two methods for constructing biased estimators that hopefully reduce total variance by a greater margin than they increase bias.</p>
<h2 id="principal-component-regression">9. Principal Component Regression</h2>
<p>One way to deal with the problem of collinearities is to appeal to the principal components of <span class="math inline">\(X\)</span>. In short, the predictors (the columns of <span class="math inline">\(X\)</span>) are replaced by the decorrelated variables <span class="math inline">\(Xq_1, \ldots, Xq_{k}\)</span> for <span class="math inline">\(k \leq r\)</span>, OLS linear regression is performed with respect to these and finally, if need be, one reverts to the initial predictors. This is called <em>principal component regression</em> (PCR).</p>
<p>Concretely, we solve the problem of finding a <span class="math inline">\(\widetilde{w}_k\)</span> that minimizes <span class="math inline">\(\|X [Q_{ij}]_{j\leq k}\, \widetilde{w}_k - y\|\)</span>. A solution is <span class="math display">\[\begin{align*}
\widetilde{w}_k &amp;= (X [Q_{ij}]_{j\leq k})^{+} y \\
&amp;= [\Sigma^{+}_{ij}]_{i\leq k} \, P^{\intercal} \, y.
\end{align*}\]</span></p>
<p>The expected value of <span class="math inline">\(\widetilde{w}_k\)</span> is <span class="math inline">\([Q_{ij}]_{j\leq k}^{\intercal} w\)</span> and its associated expected fitted value is <span class="math inline">\(X X_{k}^{+} X w\)</span>. Note that the covariance matrix of <span class="math inline">\(\widetilde{w}_{r}\)</span> is</p>
<p><span class="math display">\[\mu^{2} \Sigma^{+} (\Sigma^{+})^{\intercal}.\]</span> Furthermore, the effect of reducing <span class="math inline">\(k\)</span> below <span class="math inline">\(r\)</span> is simply to trim away the last <span class="math inline">\(r - k\)</span> entries of <span class="math inline">\(\widetilde{w}_{r}\)</span>, so the <span class="math inline">\(i^{\text{th}}\)</span> component of <span class="math inline">\(\widetilde{w}_k\)</span> (for <span class="math inline">\(i \leq k\)</span>) is independent of <span class="math inline">\(k\)</span> and its variance is <span class="math inline">\(\mu^2 \sigma_i^{-2}\)</span>. This last observation is important because it tells us that (a) we need not carry out <span class="math inline">\(r\)</span> different calculations to compute <span class="math inline">\(\widetilde{w}_1,\)</span> <span class="math inline">\(\ldots,\)</span> <span class="math inline">\(\widetilde{w}_r\)</span>, and (b) total variance can be monotonically reduced by keeping only the first few principal components of <span class="math inline">\(X\)</span>.</p>
<p>In terms of the initial predictors, retaining the first <span class="math inline">\(k\)</span> principal components is akin to using the estimator <span class="math inline">\([Q_{ij}]_{j\leq k} \, \widetilde{w}_k\)</span> for <span class="math inline">\(w\)</span>. However, unless <span class="math inline">\(X^{\intercal} X\)</span> is invertible and <span class="math inline">\(k = r\)</span>, it will generally not be unbiased, with bias equal to <span class="math display">\[[Q_{ij}]_{j\leq k} [Q_{ij}]_{j\leq k}^{\intercal}w - w.\]</span> Conceptually, the norm of the bias is the reconstruction error incurred on <span class="math inline">\(w\)</span> by the dimensionality reduction procedure seen in §6. The total variance of this estimator on the other hand is <span class="math display">\[\mu^2 \sum_{i\leq k} \sigma_i^{-2}.\]</span> As <span class="math inline">\(k\)</span> decreases so will total variance but, as one might imagine, the normed bias will generally increase.</p>
<h2 id="ridge-regression">10. Ridge Regression</h2>
<p>Instead of tempering with the matrix <span class="math inline">\(X\)</span> as PCR does, consider instead discouraging estimators from varying too dramatically by augmenting the original objective function with a penalty term. Namely, instead of looking for a <span class="math inline">\(\widehat{w}\)</span> that minimizes <span class="math inline">\(\|y - X\widehat{w}\|\)</span>, we look for a <span class="math inline">\(w&#39;\)</span> that minimizes <span class="math display">\[\|y - X w&#39;\| + \lambda \|w&#39;\|^2,\]</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is a number that can be adjusted. Calculating the gradient of this new objective function (which is still convex) and setting it to <span class="math inline">\(0\)</span>, we find that the solution is <span class="math display">\[w&#39; = (X^{\intercal} X + \lambda I)^{-1} X^{\intercal} y,\]</span> which we call the <em>ridge estimator</em>. It should be noted that the matrix <span class="math inline">\(X^{\intercal} X + \lambda I\)</span> is always invertible (why?). In terms of the SVD of <span class="math inline">\(X\)</span>, we have <span class="math display">\[\begin{align*}
  w&#39; &amp;= (Q^{\intercal} \Sigma^{\intercal} \Sigma Q + \lambda I)^{-1} Q \Sigma^{\intercal} P^{\intercal} y \\
     &amp;= Q ( \Sigma^{\intercal} \Sigma + \lambda I)^{-1} \Sigma^{\intercal} P^{\intercal} y 
\end{align*}\]</span> The expected value of the ridge estimator is <span class="math display">\[\begin{gathered}
Q (\Sigma^{\intercal} \Sigma  + \lambda I)^{-1} \Sigma^{\intercal} \Sigma Q^{\intercal} w = \\\
Q \textnormal{diag}(\tfrac{\sigma_1^{2}}{\sigma_1^{2} + \lambda}, \ldots, \tfrac{\sigma_r^{2}}{\sigma_{r}^{2} + \lambda}, 0, \ldots, 0) Q^{\intercal} w, \end{gathered}\]</span> making it evidently biased.</p>
<p>Its covariance matrix on the other hand is <span class="math display">\[\begin{gathered}
\mu^{2} Q (\Sigma^{\intercal} \Sigma + \lambda I)^{-1} \Sigma^{\intercal} \Sigma  (\Sigma^{\intercal} \Sigma + \lambda I)^{-1} Q^{\intercal} = \\\
\mu^{2} Q \textnormal{diag}(\tfrac{\sigma_1^{2}}{(\sigma_1^{2} + \lambda)^{2}}, \ldots, \tfrac{\sigma_r^{2}}{(\sigma_r^{2} + \lambda)^{2}}, 0, \ldots, 0) Q^{\intercal},\end{gathered}\]</span> giving the total variance <span class="math display">\[\mu^{2} \sum_{i\leq r} \frac{\sigma_{i}^2}{(\sigma_i^{2} + \lambda)^{2}}.\]</span> These expressions converge towards their counterparts for the OLS estimator when <span class="math inline">\(\lambda \to 0\)</span>, which tracks with the augmented objective function reverting to the original in that limit.</p>
<p>The total variance of the ridge estimator is strictly smaller than that of the OLS estimator, scaling the summand <span class="math inline">\(\sigma_i^{-2}\)</span> by a factor of <span class="math display">\[\frac{\sigma_i^{4}}{(\sigma_i^{2} + \lambda)^{2}} = \frac{1}{1   + {2\lambda} {\sigma_i^{-2}} + \lambda^{2}{\sigma_i^{-4}}},\]</span> thus attenuating <span class="math inline">\(\sigma_i^{-2}\)</span> more severely for higher values of <span class="math inline">\(i\)</span>.</p>
<p>We could therefore say that whereas PCR interpolates between the trivial estimator and the OLS estimator in steps, ridge regression does so continuously. Though PCR can cut down total variance more dramatically, ridge regression provides better guarantees on its bias: when <span class="math inline">\(X^T X\)</span> is invertible, the normed bias is bounded by <span class="math display">\[\|\textnormal{diag}(\tfrac{\lambda}{\sigma_1^{2} + \lambda}, \ldots, \tfrac{\lambda}{\sigma_1^{2} + \lambda})\| \! \cdot \! \|w\|.\]</span> In practice, ridge regression usually outperforms PCR, if only because the former represents an infinite family of estimators while the latter only a finite one.</p>
<h2 id="further-reading">11. Further Reading</h2>
<p>The book <span class="citation" data-cites="pca">[4]</span> offers a comprehensive coverage of PCA, addressing a mix of theoretical and practical questions.</p>
<p>For more theoretical results on the SVD, <span class="citation" data-cites="matrix">[3]</span> is good reference.</p>
<p>The book <span class="citation" data-cites="manning">[5]</span> and the primary sources <span class="citation" data-cites="berry index">[1, 2]</span> discuss the SVD in the context of latent semantic analysis/indexing. The reference <span class="citation" data-cites="berry">[1]</span> furthermore contains expressions for the time complexities of computing the SVD, updating the SVD, and folding-in.</p>
<p>See <span class="citation" data-cites="rec rec2">[6, 7]</span> for the use of the SVD in collaborative filtering in the context of recommender systems.</p>
<p>The link <span class="citation" data-cites="taboga">[8]</span> delves a bit more into ridge regression.</p>
<h3 class="unnumbered" id="references">References</h3>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-berry" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">Berry, M.W. et al. 1995. Using linear algebra for intelligent information retrieval. <em>SIAM review</em>. 37, 4 (1995), 573–595.</div>
</div>
<div id="ref-index" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Deerwester, S. et al. 1990. Indexing by latent semantic analysis. <em>Journal of the American society for information science</em>. 41, 6 (1990), 391–407.</div>
</div>
<div id="ref-matrix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Horn, R.A. and Johnson, C.R. 2012. <em>Matrix analysis</em>. Cambridge University Press.</div>
</div>
<div id="ref-pca" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Jolliffe, I.T. 2002. <em>Principal component analysis</em>. Springer.</div>
</div>
<div id="ref-manning" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Manning, C. 2008. <em>Introduction to information retrieval</em>. Cambridge University Press.</div>
</div>
<div id="ref-rec" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Sarwar, B. et al. 2000. <em>Application of dimensionality reduction in recommender system-a case study</em>. Minnesota Univ Minneapolis Dept of Computer Science.</div>
</div>
<div id="ref-rec2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Sarwar, B. et al. 2002. Incremental singular value decomposition algorithms for highly scalable recommender systems. <em>Fifth international conference on computer and information science</em> (2002), 27–8.</div>
</div>
<div id="ref-taboga" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Taboga, M. 2021. <a href="https://www.statlect.com/fundamentals-of-statistics/ridge-regression">Ridge regression</a>. <em>Lectures on probability theory and mathematical statistics</em>. Kindle Direct Publishing. Online appendix.</div>
</div>
</div>
<footer>
  <div class="container" style="display: flex; margin-top: 50px">
      <div class="home" width=50%>
        <a href="../index.html">Home</a>
      </div>
      <div class="published" style="flex-grow: 1; text-align: right">
      Published on Mon March 21, 2022 <br>
      Expanded on Sun Oct 23, 2022
      </div>
  </div>
</footer>
</body>
</html>
